@techreport{dgx-1,
     title = {NVIDIA DGX-1 With Tesla
V100 System Architecture},
     year = {2017},
     institution = {NVIDIA}
}

@article{wangchanberta2021,
  author    = {Lalita Lowphansirikul and
               Charin Polpanumas and
               Nawat Jantrakulchai and
               Sarana Nutanong},
  title     = {WangchanBERTa: Pretraining transformer-based Thai Language Models},
  journal   = {CoRR},
  volume    = {abs/2101.09635},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.09635},
  eprinttype = {arXiv},
  eprint    = {2101.09635},
  timestamp = {Sat, 30 Jan 2021 18:02:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-09635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tobias-stock,
	Art_Number = {093024},
	Author = {Tobias Preis and Peter Virnau and Wolfgang Paul and Johannes J. Schneider},
	Journal = {New Journal of Physics},
	Title = {Accelerated fluctuation analysis by graphic cards and complex pattern formation in financial markets},
	Volume = {11},
	Year = {2009}}


@inproceedings{sms,
 author = {Ausavarungnirun, Rachata and Chang, Kevin Kai-Wei and Subramanian, Lavanya and Loh, Gabriel and Mutlu, Onur},
 title = {Staged Memory Scheduling: Achieving High Performance and Scalability in Heterogeneous Systems},
 booktitle = {ISCA},
 year = {2012},
} 

@article{paper1,
 author = {Ausavarungnirun, Rachata and Chang, Kevin Kai-Wei and Subramanian, Lavanya and Loh, Gabriel H. and Mutlu, Onur},
 title = {Staged Memory Scheduling: Achieving High Performance and Scalability in Heterogeneous Systems},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {June 2012},
 volume = {40},
 number = {3},
 month = jun,
 year = {2012},
 issn = {0163-5964},
 pages = {416--427},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2366231.2337207},
 doi = {10.1145/2366231.2337207},
 acmid = {2337207},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{radford2019better,
  title={Better language models and their implications},
  author={Radford, Alec and Wu, Jeffrey and Amodei, Dario and Amodei, Daniela and Clark, Jack and Brundage, Miles and Sutskever, Ilya},
  journal={OpenAI Blog https://openai.com/blog/better-language-models},
  volume={1},
  pages={2},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{brownlee2017gentle,
  title={A gentle introduction to transfer learning for deep learning},
  author={Brownlee, Jason},
  journal={Machine Learning Mastery},
  volume={20},
  year={2017},
  url = {https://machinelearningmastery.com/transfer-learning-for-deep-learning/}
}