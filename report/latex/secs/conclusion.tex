\section{Conclusion} \label{sec:conclusion}

We investigate the kernel/interface calling from the training process. The results show that as the number of GPUs increases, the communication overhead is also introduced to the system. It means that the horizontal scaling for GPU training is highly non-linear. In addition, we also found an asymmetry in spending time on memory management regarding the multi-GPUs regime.
